{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "(A,terms,snippets) = joblib.load( \"articles-raw.pkl\" )\n",
    "print( \"Loaded %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin, kmax = 4, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "topic_models = []\n",
    "# try each value of k\n",
    "for k in range(kmin,kmax+1):\n",
    "    print(\"Applying LDA for k=%d ...\" % k )\n",
    "    # run LDA\n",
    "    model = LatentDirichletAllocation(n_components=k, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(A)\n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_    \n",
    "    # store for later\n",
    "    topic_models.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "raw_documents = []\n",
    "with open( \"dogu.csv\" ,\"r\", encoding=\"utf8\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        raw_documents.append( line.strip().lower() )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\",  ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip().lower() )\n",
    "# note that we need to make it hashable\n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class TokenGenerator:\n",
    "    def __init__( self, documents, stopwords ):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n",
    "\n",
    "    def __iter__( self ):\n",
    "        print(\"Building Word2Vec model ...\")\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall( doc ):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append( \"<stopword>\" )\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append( tok )\n",
    "            yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "docgen = TokenGenerator( raw_documents, custom_stop_words )\n",
    "# the model has 500 dimensions, the minimum document-term frequency is 20\n",
    "w2v_model = gensim.models.Word2Vec(docgen, vector_size=500, min_count=20, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"w2v-model-lda.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( w2v_model.wv.similarity(pair[0], pair[1]) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_descriptor( all_terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( all_terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append(k)\n",
    "    coherences.append(calculate_coherence(w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "matplotlib.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,7))\n",
    "# create the line plot\n",
    "ax = plt.plot( k_values, coherences )\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Mean Coherence\")\n",
    "# add the points\n",
    "plt.scatter( k_values, coherences, s=120)\n",
    "# find and annotate the maximum point on the plot\n",
    "ymax = max(coherences)\n",
    "xpos = coherences.index(ymax)\n",
    "best_k = k_values[xpos]\n",
    "plt.annotate( \"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = best_k\n",
    "# get the model that we generated earlier.\n",
    "W = topic_models[k-kmin][1]\n",
    "H = topic_models[k-kmin][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_index in range(k):\n",
    "    descriptor = get_descriptor( terms, H, topic_index, 10 )\n",
    "    str_descriptor = \", \".join( descriptor )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
